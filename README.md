
# ğŸ¯ PERSONAL_AIRFLOW

> **"ë°ì´í„° íë¦„ ì „ì²´ë¥¼ ìë™í™”í•˜ê³ , ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•˜ë‹¤."**

ì´ í”„ë¡œì íŠ¸ëŠ” íšŒì› ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ **ETL ìë™í™” ë° ì‚¬ìš©ì ë¶„ì„ íŒŒì´í”„ë¼ì¸**ì…ë‹ˆë‹¤.
Airflow ê¸°ë°˜ì˜ DAGìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, **AWS S3 â†’ ë°ì´í„° ì „ì²˜ë¦¬ â†’ ë¶„ì„ â†’ BigQuery ì ì¬** ì „ ê³¼ì •ì„ ìë™í™”í•©ë‹ˆë‹¤.

ë˜í•œ GA4 ë¡œê·¸ë¥¼ í•¨ê»˜ í™œìš©í•˜ì—¬, **ì„œë¹„ìŠ¤ ì°¸ì—¬ë„ ì§€í‘œ(WAU/MAU)** ì™€ **ì „í™˜ìœ¨ ë¶„ì„**ê¹Œì§€ ì—°ê²°í•©ë‹ˆë‹¤.

---

## ğŸ§© êµ¬ì„± ìš”ì†Œ

* **Airflow DAG**: ë¶„ì„ íë¦„ ì „ë°˜ì„ ì •ì˜ (`Analysis_latest.py`)
* **S3 ì—°ë™**: ë©¤ë²„ì‹­ CSVë¥¼ ë¡œë“œí•˜ì—¬ ì „ì²˜ë¦¬
* **ì‚¬ìš©ì ë¶„ì„**: ê°€ì…, ê²°ì œ, ì´íƒˆ, ë¦¬í…ì…˜ ì§€í‘œ ê³„ì‚°
* **GA4 ê¸°ë°˜ ë¶„ì„**: ì™¸ë¶€ ì¿¼ë¦¬(`.sql`)ë¥¼ í†µí•´ ì£¼ìš” ì§€í‘œ ì¶”ì¶œ
* **BigQuery ì ì¬**: ë¶„ì„ ê²°ê³¼ë¥¼ ì£¼ê°„/ì›”ê°„ í…Œì´ë¸”ë¡œ ì €ì¥

---

## ğŸ—‚ í´ë” êµ¬ì¡°

```
PERSONAL_AIRFLOW/
â”œâ”€â”€ dags/              # DAG ì •ì˜
â”‚   â”œâ”€â”€ analysis_latest.py
â”œâ”€â”€ queries/           # ì™¸ë¶€ SQL (GA4 ì¿¼ë¦¬)
â”‚   â”œâ”€â”€ mau_query.sql
â”‚   â””â”€â”€ wau_query.sql
â”œâ”€â”€ scripts/           # ì—°ê²° ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ init_airflow_connections.sh
â”œâ”€â”€ config/            # ì—°ê²° ì •ë³´ (ì˜ˆì‹œ)
â”œâ”€â”€ auth/              # ì¸ì¦ íŒŒì¼ (ì—…ë¡œë“œ ì œì™¸)
â”œâ”€â”€ plugins/           # ì»¤ìŠ¤í…€ ì˜¤í¼ë ˆì´í„° (ì˜µì…˜)
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ entrypoint.sh
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

> í•´ë‹¹ ë ˆí¬ì§€í† ë¦¬ì—ì„œëŠ” `dags/`, `queries/`, `requirements.txt` ë§Œì„ í¬í•¨í–ˆìŠµë‹ˆë‹¤.  

---

## ğŸ” ë°ì´í„° íë¦„ êµ¬ì¡°ë„

ì‹¤ì œ Airflow DAG ì‹¤í–‰ ê²°ê³¼:
![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-03-19 11 13 18](https://github.com/user-attachments/assets/c362c2be-0376-4d9b-9a56-8d2d6265358d)

Mermaid ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œë„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```mermaid
graph TB
  A[S3: CSV ì—…ë¡œë“œ] --> B[Extract Task]
  B --> C[Datetime ì „ì²˜ë¦¬]
  C --> D1[ì£¼ê°„ ë¶„ì„]
  C --> D2[ì›”ê°„ ë¶„ì„]
  D1 --> E1[GA4 WAU ì¿¼ë¦¬ ì‹¤í–‰]
  D2 --> E2[GA4 MAU ì¿¼ë¦¬ ì‹¤í–‰]
  E1 --> F1[ì „í™˜ìœ¨ ê³„ì‚°]
  E2 --> F2[ì „í™˜ìœ¨ ê³„ì‚°]
  F1 --> G1[BigQuery ì ì¬: Weekly]
  F2 --> G2[BigQuery ì ì¬: Monthly]
```

---

## ğŸ›  ê¸°ìˆ  ìŠ¤íƒ

<!--Python-->
<img src="https://img.shields.io/badge/Python-3776AB?style=rounded&logo=Python&logoColor=white" height="25"/> <!--Apache Airflow--> <img src="https://img.shields.io/badge/Airflow-017CEE?style=rounded&logo=Apache%20Airflow&logoColor=white" height="25"/> <!--Amazon S3--> <img src="https://img.shields.io/badge/Amazon%20S3-569A31?style=rounded&logo=Amazon%20S3&logoColor=white" height="25"/> <!--Google BigQuery--> <img src="https://img.shields.io/badge/BigQuery-1A73E8?style=rounded&logo=Google%20BigQuery&logoColor=white" height="25"/> <!--Docker--> <img src="https://img.shields.io/badge/Docker-0db7ed?style=rounded&logo=Docker&logoColor=white" height="25"/> <!--SQL--> <img src="https://img.shields.io/badge/SQL-4479A1?style=rounded&logo=SQLite&logoColor=white" height="25"/> <img src="https://img.shields.io/badge/%2B%20more-8E44AD?style=rounded&logoColor=white" height="25"/>


---

## âœï¸ ê¸°íš ì˜ë„

* ë°˜ë³µì ì¸ ë¶„ì„ ê³¼ì •ì„ ìë™í™”í•˜ê³  ì‹¶ë‹¤ëŠ” í•„ìš”ì—ì„œ ì¶œë°œí•œ ê°œì¸ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.
* ë°ì´í„° ì „ì²˜ë¦¬ë¶€í„° ë¡œê·¸ ê¸°ë°˜ ë¶„ì„, ì „í™˜ìœ¨ ê³„ì‚°ê¹Œì§€ **ë¶„ì„-ì—”ì§€ë‹ˆì–´ë§ì˜ ì—°ê²° íë¦„**ì„ ê³ ë ¤í•´ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.
* ì‹¤ë¬´ì—ì„œ ì‚¬ìš©í•´ì˜¨ Apache Airflowë¥¼ í™œìš©í•˜ì—¬, **íƒœìŠ¤í¬ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³  ì—°ê²°í•˜ëŠ” ë°©ì‹**ì„ ì •ì œí•˜ê³ ì í–ˆìŠµë‹ˆë‹¤.
* ê¸°ì¡´ ì½”ë“œì™€ íë¦„ì„ ëŒì•„ë³´ë©°, **ì¬ì‚¬ìš©ì„±ê³¼ í™•ì¥ì„±ì„ ê³ ë ¤í•œ êµ¬ì¡°ë¡œ ê°œì„ **í•˜ëŠ” ë° ì¤‘ì ì„ ë‘ì—ˆìŠµë‹ˆë‹¤.
<br/>
<br/>
<br/>

---
<br/>
<br/>

# ğŸ¯ PERSONAL_AIRFLOW

> **"Automating the full flow of dataâ€”from raw logs to business insights."**

This project implements an automated **data pipeline and user analytics workflow** based on membership data.
Using Apache Airflow, it processes CSV files from **AWS S3**, performs time-based transformations, computes **weekly/monthly user metrics**, and stores results in **Google BigQuery**.

In addition, it integrates GA4 log data through external SQL queries to track **WAU/MAU** and **conversion trends**.

---

## ğŸ§© Key Components

* **Airflow DAG**: Full pipeline orchestration (`Analysis_latest.py`)
* **S3 Integration**: Loads raw membership CSV files
* **User Analytics**: Tracks signup, payment, churn, and retention KPIs
* **GA4 Insights**: Uses prewritten `.sql` queries to extract behavioral metrics
* **BigQuery Load**: Uploads results to weekly/monthly analytics tables

---

## ğŸ—‚ Folder Structure

```
PERSONAL_AIRFLOW/
â”œâ”€â”€ dags/              # DAG definition
â”œâ”€â”€ queries/           # External SQL queries (GA4)
â”œâ”€â”€ scripts/           # Initialization scripts
â”œâ”€â”€ config/            # Connection setup (example)
â”œâ”€â”€ auth/              # Authentication (excluded)
â”œâ”€â”€ plugins/           # Custom operators (optional)
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ” Data Flow Diagram

```mermaid
graph TD
  A[S3: Raw CSV] --> B[Extract Task]
  B --> C[Datetime Preprocessing]
  C --> D1[Weekly Analysis]
  C --> D2[Monthly Analysis]
  D1 --> E1[WAU SQL Query]
  D2 --> E2[MAU SQL Query]
  E1 --> F1[Join + Conversion Metrics]
  E2 --> F2[Join + Conversion Metrics]
  F1 --> G1[BigQuery Load: Weekly]
  F2 --> G2[BigQuery Load: Monthly]
```

---

## ğŸ›  Tech Stack
  
<!--Python-->
<img src="https://img.shields.io/badge/Python-3776AB?style=rounded&logo=Python&logoColor=white" height="25"/> <!--Apache Airflow--> <img src="https://img.shields.io/badge/Airflow-017CEE?style=rounded&logo=Apache%20Airflow&logoColor=white" height="25"/> <!--Amazon S3--> <img src="https://img.shields.io/badge/Amazon%20S3-569A31?style=rounded&logo=Amazon%20S3&logoColor=white" height="25"/> <!--Google BigQuery--> <img src="https://img.shields.io/badge/BigQuery-1A73E8?style=rounded&logo=Google%20BigQuery&logoColor=white" height="25"/> <!--Docker--> <img src="https://img.shields.io/badge/Docker-0db7ed?style=rounded&logo=Docker&logoColor=white" height="25"/> <!--SQL--> <img src="https://img.shields.io/badge/SQL-4479A1?style=rounded&logo=SQLite&logoColor=white" height="25"/> <img src="https://img.shields.io/badge/%2B%20more-8E44AD?style=rounded&logoColor=white" height="25"/>

---

## âœï¸ Project Motivation

* This project started from a personal need to automate repetitive data analysis tasks.
* It covers the entire flow from data preprocessing to log-based analysis and conversion rate calculation, aiming to **bridge the gap between analytics and engineering**.
* Using Apache Airflowâ€”previously applied in real-world tasksâ€”I focused on organizing the process into clear, task-oriented steps.
* Throughout the project, I aimed to **refactor the workflow with a focus on reusability and scalability**, while reflecting on and improving past implementations.
