
# üéØ PERSONAL_AIRFLOW

> **"Îç∞Ïù¥ÌÑ∞ ÌùêÎ¶Ñ Ï†ÑÏ≤¥Î•º ÏûêÎèôÌôîÌïòÍ≥†, Ïù∏ÏÇ¨Ïù¥Ìä∏Î•º Ï∂îÏ∂úÌïòÎã§."**

Ïù¥ ÌîÑÎ°úÏ†ùÌä∏Îäî ÌöåÏõê Îç∞Ïù¥ÌÑ∞Î•º Í∏∞Î∞òÏúºÎ°ú Ìïú **ETL ÏûêÎèôÌôî Î∞è ÏÇ¨Ïö©Ïûê Î∂ÑÏÑù ÌååÏù¥ÌîÑÎùºÏù∏**ÏûÖÎãàÎã§.
Airflow Í∏∞Î∞òÏùò DAGÏúºÎ°ú Íµ¨ÏÑ±ÎêòÏñ¥ ÏûàÏúºÎ©∞, **AWS S3 ‚Üí Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ ‚Üí Î∂ÑÏÑù ‚Üí BigQuery Ï†ÅÏû¨** Ï†Ñ Í≥ºÏ†ïÏùÑ ÏûêÎèôÌôîÌï©ÎãàÎã§.

ÎòêÌïú GA4 Î°úÍ∑∏Î•º Ìï®Íªò ÌôúÏö©ÌïòÏó¨, **ÏÑúÎπÑÏä§ Ï∞∏Ïó¨ÎèÑ ÏßÄÌëú(WAU/MAU)** ÏôÄ **Ï†ÑÌôòÏú® Î∂ÑÏÑù**ÍπåÏßÄ Ïó∞Í≤∞Ìï©ÎãàÎã§.

---

## üß© Íµ¨ÏÑ± ÏöîÏÜå

* **Airflow DAG**: Î∂ÑÏÑù ÌùêÎ¶Ñ Ï†ÑÎ∞òÏùÑ Ï†ïÏùò (`Analysis_latest.py`)
* **S3 Ïó∞Îèô**: Î©§Î≤ÑÏã≠ CSVÎ•º Î°úÎìúÌïòÏó¨ Ï†ÑÏ≤òÎ¶¨
* **ÏÇ¨Ïö©Ïûê Î∂ÑÏÑù**: Í∞ÄÏûÖ, Í≤∞Ï†ú, Ïù¥ÌÉà, Î¶¨ÌÖêÏÖò ÏßÄÌëú Í≥ÑÏÇ∞
* **GA4 Í∏∞Î∞ò Î∂ÑÏÑù**: Ïô∏Î∂Ä ÏøºÎ¶¨(`.sql`)Î•º ÌÜµÌï¥ Ï£ºÏöî ÏßÄÌëú Ï∂îÏ∂ú
* **BigQuery Ï†ÅÏû¨**: Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Ï£ºÍ∞Ñ/ÏõîÍ∞Ñ ÌÖåÏù¥Î∏îÎ°ú Ï†ÄÏû•

---

## üóÇ Ìè¥Îçî Íµ¨Ï°∞

```
PERSONAL_AIRFLOW/
‚îú‚îÄ‚îÄ dags/              # DAG Ï†ïÏùò
‚îú‚îÄ‚îÄ queries/           # Ïô∏Î∂Ä SQL (GA4 ÏøºÎ¶¨)
‚îú‚îÄ‚îÄ scripts/           # Ïó∞Í≤∞ Ï¥àÍ∏∞Ìôî Ïä§ÌÅ¨Î¶ΩÌä∏
‚îú‚îÄ‚îÄ config/            # Ïó∞Í≤∞ Ï†ïÎ≥¥ (ÏòàÏãú)
‚îú‚îÄ‚îÄ auth/              # Ïù∏Ï¶ù ÌååÏùº (ÏóÖÎ°úÎìú Ï†úÏô∏)
‚îú‚îÄ‚îÄ plugins/           # Ïª§Ïä§ÌÖÄ Ïò§ÌçºÎ†àÏù¥ÌÑ∞ (ÏòµÏÖò)
‚îú‚îÄ‚îÄ docker-compose.yaml
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üîÅ Îç∞Ïù¥ÌÑ∞ ÌùêÎ¶Ñ Íµ¨Ï°∞ÎèÑ

```mermaid
graph TD
  A[S3: CSV ÏóÖÎ°úÎìú] --> B[Extract Task]
  B --> C[Datetime Ï†ÑÏ≤òÎ¶¨]
  C --> D1[Ï£ºÍ∞Ñ Î∂ÑÏÑù]
  C --> D2[ÏõîÍ∞Ñ Î∂ÑÏÑù]
  D1 --> E1[GA4 WAU ÏøºÎ¶¨ Ïã§Ìñâ]
  D2 --> E2[GA4 MAU ÏøºÎ¶¨ Ïã§Ìñâ]
  E1 --> F1[Ï†ÑÌôòÏú® Í≥ÑÏÇ∞]
  E2 --> F2[Ï†ÑÌôòÏú® Í≥ÑÏÇ∞]
  F1 --> G1[BigQuery Ï†ÅÏû¨: Weekly]
  F2 --> G2[BigQuery Ï†ÅÏû¨: Monthly]
```

---

## üõ† Í∏∞Ïà† Ïä§ÌÉù

* **Python**, Pandas, NumPy
* **Apache Airflow**
* **AWS S3**
* **Google BigQuery**
* SQL (GA4 ÏøºÎ¶¨)
* Docker

---

## ‚úçÔ∏è Í∏∞Ìöç ÏùòÎèÑ

* Î∞òÎ≥µÏ†ÅÏù∏ Î∂ÑÏÑù Í≥ºÏ†ïÏùÑ ÏûêÎèôÌôîÌïòÍ≥† Ïã∂Îã§Îäî ÌïÑÏöîÏóêÏÑú Ï∂úÎ∞úÌïú Í∞úÏù∏ ÌîÑÎ°úÏ†ùÌä∏ÏûÖÎãàÎã§.
* Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨Î∂ÄÌÑ∞ Î°úÍ∑∏ Í∏∞Î∞ò Î∂ÑÏÑù, Ï†ÑÌôòÏú® Í≥ÑÏÇ∞ÍπåÏßÄ **Î∂ÑÏÑù-ÏóîÏßÄÎãàÏñ¥ÎßÅÏùò Ïó∞Í≤∞ ÌùêÎ¶Ñ**ÏùÑ Í≥†Î†§Ìï¥ Íµ¨ÏÑ±ÌñàÏäµÎãàÎã§.
* Ïã§Î¨¥ÏóêÏÑú ÏÇ¨Ïö©Ìï¥Ïò® Apache AirflowÎ•º ÌôúÏö©ÌïòÏó¨, **ÌÉúÏä§ÌÅ¨ Îã®ÏúÑÎ°ú ÎÇòÎàÑÍ≥† Ïó∞Í≤∞ÌïòÎäî Î∞©Ïãù**ÏùÑ Ï†ïÏ†úÌïòÍ≥†Ïûê ÌñàÏäµÎãàÎã§.
* Í∏∞Ï°¥ ÏΩîÎìúÏôÄ ÌùêÎ¶ÑÏùÑ ÎèåÏïÑÎ≥¥Î©∞, **Ïû¨ÏÇ¨Ïö©ÏÑ±Í≥º ÌôïÏû•ÏÑ±ÏùÑ Í≥†Î†§Ìïú Íµ¨Ï°∞Î°ú Í∞úÏÑ†**ÌïòÎäî Îç∞ Ï§ëÏ†êÏùÑ ÎëêÏóàÏäµÎãàÎã§.

---

# üéØ PERSONAL_AIRFLOW

> **"Automating the full flow of data‚Äîfrom raw logs to business insights."**

This project implements an automated **data pipeline and user analytics workflow** based on membership data.
Using Apache Airflow, it processes CSV files from **AWS S3**, performs time-based transformations, computes **weekly/monthly user metrics**, and stores results in **Google BigQuery**.

In addition, it integrates GA4 log data through external SQL queries to track **WAU/MAU** and **conversion trends**.

---

## üß© Key Components

* **Airflow DAG**: Full pipeline orchestration (`Analysis_latest.py`)
* **S3 Integration**: Loads raw membership CSV files
* **User Analytics**: Tracks signup, payment, churn, and retention KPIs
* **GA4 Insights**: Uses prewritten `.sql` queries to extract behavioral metrics
* **BigQuery Load**: Uploads results to weekly/monthly analytics tables

---

## üóÇ Folder Structure

```
PERSONAL_AIRFLOW/
‚îú‚îÄ‚îÄ dags/              # DAG definition
‚îú‚îÄ‚îÄ queries/           # External SQL queries (GA4)
‚îú‚îÄ‚îÄ scripts/           # Initialization scripts
‚îú‚îÄ‚îÄ config/            # Connection setup (example)
‚îú‚îÄ‚îÄ auth/              # Authentication (excluded)
‚îú‚îÄ‚îÄ plugins/           # Custom operators (optional)
‚îú‚îÄ‚îÄ docker-compose.yaml
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üîÅ Data Flow Diagram

```mermaid
graph TD
  A[S3: Raw CSV] --> B[Extract Task]
  B --> C[Datetime Preprocessing]
  C --> D1[Weekly Analysis]
  C --> D2[Monthly Analysis]
  D1 --> E1[WAU SQL Query]
  D2 --> E2[MAU SQL Query]
  E1 --> F1[Join + Conversion Metrics]
  E2 --> F2[Join + Conversion Metrics]
  F1 --> G1[BigQuery Load: Weekly]
  F2 --> G2[BigQuery Load: Monthly]
```

---

## üõ† Tech Stack

* **Python**, Pandas, NumPy
* **Apache Airflow**
* **AWS S3**
* **Google BigQuery**
* SQL (Google Analytics 4)
* Docker

---

## ‚úçÔ∏è Project Motivation

* This project started from a personal need to automate repetitive data analysis tasks.
* It covers the entire flow from data preprocessing to log-based analysis and conversion rate calculation, aiming to **bridge the gap between analytics and engineering**.
* Using Apache Airflow‚Äîpreviously applied in real-world tasks‚ÄîI focused on organizing the process into clear, task-oriented steps.
* Throughout the project, I aimed to **refactor the workflow with a focus on reusability and scalability**, while reflecting on and improving past implementations.
