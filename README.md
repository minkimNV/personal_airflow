<br/>
<div align="center">
  <sub>
  ë³¸ ë ˆí¬ì§€í† ë¦¬ëŠ” íšŒì› ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ <b>ETL ìë™í™” ë° ì‚¬ìš©ì ë¶„ì„ íŒŒì´í”„ë¼ì¸</b>ì„ ì •ë¦¬í•œ ê°œì¸ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. Apache Airflowë¥¼ í™œìš©í•´ ì‹¤ë¬´ì—ì„œ ë°˜ë³µë˜ë˜ ë¶„ì„ ì—…ë¬´ë¥¼ ìë™í™”í•˜ê³ , ë¡œê·¸ ê¸°ë°˜ ì§€í‘œ ë¶„ì„ê¹Œì§€ ì—°ê²°í•˜ëŠ” íë¦„ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.  
  (This repository is a personal project that outlines an automated ETL and user analytics pipeline based on membership data. It leverages Apache Airflow to streamline repetitive analytical tasks and connects the workflow with log-based metric analysis.)
  </sub>  
</div>
<br/>
<br/>

---

# ğŸ¯ PERSONAL_AIRFLOW

> **"ë°ì´í„° íë¦„ ì „ì²´ë¥¼ ìë™í™”í•˜ê³ , ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•˜ë‹¤."**

ì´ í”„ë¡œì íŠ¸ëŠ” íšŒì› ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ **ETL ìë™í™” ë° ì‚¬ìš©ì ë¶„ì„ íŒŒì´í”„ë¼ì¸**ì…ë‹ˆë‹¤.
Airflow ê¸°ë°˜ì˜ DAGìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, **AWS S3 â†’ ë°ì´í„° ì „ì²˜ë¦¬ â†’ ë¶„ì„ â†’ BigQuery ì ì¬** ì „ ê³¼ì •ì„ ìë™í™”í•©ë‹ˆë‹¤.

ë˜í•œ GA4 ë¡œê·¸ë¥¼ í•¨ê»˜ í™œìš©í•˜ì—¬, **ì„œë¹„ìŠ¤ ì°¸ì—¬ë„ ì§€í‘œ(WAU/MAU)** ì™€ **ì „í™˜ìœ¨ ë¶„ì„**ê¹Œì§€ ì—°ê²°í•©ë‹ˆë‹¤.

<br/>

---

## ğŸ§© êµ¬ì„± ìš”ì†Œ

* **Airflow DAG**: ì „ì²´ ë¶„ì„ íë¦„ì„ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ (`service_data_pipeline.py`)
* **S3 ì—°ë™**: ë©¤ë²„ì‹­ CSVë¥¼ ë¡œë“œí•˜ì—¬ ì „ì²˜ë¦¬
* **ì‚¬ìš©ì ë¶„ì„**: ê°€ì…, ê²°ì œ, ì´íƒˆ, ë¦¬í…ì…˜ ì§€í‘œ ê³„ì‚°
* **GA4 ê¸°ë°˜ ë¶„ì„**: ì™¸ë¶€ ì¿¼ë¦¬(`.sql`)ë¥¼ í†µí•´ ì£¼ìš” ì§€í‘œ ì¶”ì¶œ
* **BigQuery ì ì¬**: ë¶„ì„ ê²°ê³¼ë¥¼ ì£¼ê°„/ì›”ê°„ í…Œì´ë¸”ë¡œ ì €ì¥

<br/>

---

## ğŸ—‚ í´ë” êµ¬ì¡°

```
PERSONAL_AIRFLOW/
â”œâ”€â”€ dags/              # DAG ì •ì˜
â”‚   â”œâ”€â”€ blog_data_pipeline.py
â”‚   â”œâ”€â”€ blog_member_management.py
â”‚   â””â”€â”€ service_data_pipeline.py
â”œâ”€â”€ queries/           # ì™¸ë¶€ SQL (GA4 ì¿¼ë¦¬)
â”‚   â”œâ”€â”€ mau_query.sql
â”‚   â””â”€â”€ wau_query.sql
â”œâ”€â”€ scripts/           # ì—°ê²° ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ init_airflow_connections.sh
â”œâ”€â”€ config/            # ì—°ê²° ì •ë³´ (ì˜ˆì‹œ)
â”œâ”€â”€ auth/              # ì¸ì¦ íŒŒì¼ (ì—…ë¡œë“œ ì œì™¸)
â”œâ”€â”€ plugins/           # ì»¤ìŠ¤í…€ ì˜¤í¼ë ˆì´í„° (ì˜µì…˜)
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ entrypoint.sh
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

> í•´ë‹¹ ë ˆí¬ì§€í† ë¦¬ì—ì„œëŠ” `dags/`, `queries/`, `requirements.txt` ë§Œì„ í¬í•¨í–ˆìŠµë‹ˆë‹¤.  

<br/>

---

## ğŸ” ë°ì´í„° íë¦„ êµ¬ì¡°ë„

ì‹¤ì œ Airflow DAG ì‹¤í–‰ ê²°ê³¼:
![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-03-19 11 13 18](https://github.com/user-attachments/assets/c362c2be-0376-4d9b-9a56-8d2d6265358d)

<br/>

Mermaid ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œë„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```mermaid
graph TB
  A[S3: CSV ì—…ë¡œë“œ] --> B[Extract Task]
  B --> C[Datetime ì „ì²˜ë¦¬]
  C --> D1[ì£¼ê°„ ë¶„ì„]
  C --> D2[ì›”ê°„ ë¶„ì„]
  D1 --> E1[GA4 WAU ì¿¼ë¦¬ ì‹¤í–‰]
  D2 --> E2[GA4 MAU ì¿¼ë¦¬ ì‹¤í–‰]
  E1 --> F1[ì „í™˜ìœ¨ ë¶„ì„]
  E2 --> F2[ì „í™˜ìœ¨ ë¶„ì„]
  F1 --> G1[BigQuery ì ì¬: Weekly]
  F2 --> G2[BigQuery ì ì¬: Monthly]
```

<br/>

### ğŸ’¡ ì£¼ìš” íƒœìŠ¤í¬ ìš”ì•½ (task_id ê¸°ì¤€)

| Task ID                  | ì„¤ëª… |
|--------------------------|------|
| `extract`                | S3ì—ì„œ ì¼ë³„ íšŒì› CSV ë°ì´í„° ë¡œë“œ |
| `time_setting`           | ë‚ ì§œ ì»¬ëŸ¼ ì •ë¦¬ ë° ì£¼/ì›” ê¸°ì¤€ ì»¬ëŸ¼ ì¶”ê°€ |
| `weekly_user_analysis`   | ì£¼ì°¨ ê¸°ì¤€ ìœ ì € ì§€í‘œ ê³„ì‚° (ê°€ì…/ìœ ë£Œ/ì´íƒˆ ë“±) |
| `weekly_service_analysis`| GA4 ë¡œê·¸ ê¸°ë°˜ WAU ë¶„ì„ ë° ìœ ì € ë°ì´í„° ë³‘í•© |
| `monthly_user_analysis`  | ì›” ê¸°ì¤€ ìœ ì € ì§€í‘œ ê³„ì‚° |
| `monthly_service_analysis`| GA4 ë¡œê·¸ ê¸°ë°˜ MAU ë¶„ì„ ë° ë³‘í•© |
| `load_weekly_report`     | ì£¼ê°„ ë¶„ì„ ê²°ê³¼ BigQuery í…Œì´ë¸”ì— ì €ì¥ |
| `load_monthly_report`    | ì›”ê°„ ë¶„ì„ ê²°ê³¼ BigQuery í…Œì´ë¸”ì— ì €ì¥ |


<br/>

---

## ğŸ›  ê¸°ìˆ  ìŠ¤íƒ

<!--Python-->
<img src="https://img.shields.io/badge/Python-3776AB?style=rounded&logo=Python&logoColor=white" height="25"/> <!--Apache Airflow--> <img src="https://img.shields.io/badge/Airflow-017CEE?style=rounded&logo=Apache%20Airflow&logoColor=white" height="25"/> <!--Amazon S3--> <img src="https://img.shields.io/badge/Amazon%20S3-569A31?style=rounded&logo=Amazon%20S3&logoColor=white" height="25"/> <!--Google BigQuery--> <img src="https://img.shields.io/badge/BigQuery-1A73E8?style=rounded&logo=Google%20BigQuery&logoColor=white" height="25"/> <!--Docker--> <img src="https://img.shields.io/badge/Docker-0db7ed?style=rounded&logo=Docker&logoColor=white" height="25"/> <!--SQL--> <img src="https://img.shields.io/badge/SQL-4479A1?style=rounded&logo=SQLite&logoColor=white" height="25"/> <img src="https://img.shields.io/badge/%2B%20more-8E44AD?style=rounded&logoColor=white" height="25"/>

<br/>
---

## ğŸ“¦ ì¶”ê°€ DAG ì†Œê°œ

### `blog_data_pipeline.py`
> ë¸”ë¡œê·¸ êµ¬ë…ì(members)ì™€ ë‰´ìŠ¤ë ˆí„°(posts) ë°ì´í„°ë¥¼ Ghost Admin APIì—ì„œ ìˆ˜ì§‘í•˜ê³ ,  
> ì£¼ê°„ ë‹¨ìœ„ë¡œ ì£¼ìš” ì§€í‘œ(ê°€ì…ì ìˆ˜, í™œì„± ìœ ì € ìˆ˜, êµ¬ë…ë¥ , ë‰´ìŠ¤ë ˆí„° í´ë¦­/ì˜¤í”ˆìœ¨ ë“±)ë¥¼ ì‚°ì¶œí•©ë‹ˆë‹¤.  
> - ìˆ˜ì§‘ëœ ë°ì´í„°ëŠ” S3 ë° BigQueryì— ì €ì¥ë˜ì–´ ì¶”í›„ ë¦¬í¬íŠ¸ ë° ëŒ€ì‹œë³´ë“œ êµ¬ì¶•ì— í™œìš©ë©ë‹ˆë‹¤.  
> - í•œêµ­ì–´/ì˜ì–´ êµ¬ë…ìë¥¼ ë¶„ë¦¬í•˜ì—¬ ë¶„ì„í•˜ë©°, ì–¸ì–´ë³„ KPIë¥¼ ë³‘ë ¬ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.

### `blog_member_management.py`
> Ghost ë¸”ë¡œê·¸ ë©¤ë²„ ì‹œìŠ¤í…œì„ ê´€ë¦¬í•˜ëŠ” ìë™í™” íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.  
> - ë¹„í™œì„±/ìŠ¤íŒ¸ì„± ìœ ì €ë¥¼ ìë™ ì‚­ì œí•˜ê³ ,  
> - BigQueryì—ì„œ ì¶”ì²œëœ ë³´ë¥˜(pending) ìœ ì €ë¥¼ Ghostì— ìë™ ìƒì„±í•©ë‹ˆë‹¤.  
> - Ghost ì‹œìŠ¤í…œê³¼ BigQuery í…Œì´ë¸” ê°„ì˜ ì •í•©ì„±ì„ ì£¼ê¸°ì ìœ¼ë¡œ ë§ì¶”ë©° ë™ê¸°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

<br/>

---

## âœï¸ ê¸°íš ì˜ë„

* ì‹¤ë¬´ì—ì„œ ë°˜ë³µë˜ëŠ” ë¶„ì„ ì—…ë¬´ë¥¼ ë” íš¨ìœ¨ì ìœ¼ë¡œ ë‹¤ë£¨ê¸° ìœ„í•´, ê°œì¸ì ìœ¼ë¡œ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.
* Apache Airflowë¥¼ í™œìš©í•´ ë¶„ì„ê³¼ ì—”ì§€ë‹ˆì–´ë§ì˜ ê²½ê³„ë¥¼ ì—°ê²°í•˜ê³ , ì „ì²´ íë¦„ì„ ì§ì ‘ ì„¤ê³„í•˜ê³  ìë™í™”í•´ë³¸ ê²½í—˜ì„ ì •ë¦¬í•œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.
* ë°ì´í„° ì „ì²˜ë¦¬ë¶€í„° ë¡œê·¸ ê¸°ë°˜ ë¶„ì„, ì „í™˜ìœ¨ ê³„ì‚°ê¹Œì§€ **ë¶„ì„â€“ì—”ì§€ë‹ˆì–´ë§ ê°„ ìœ ê¸°ì ì¸ íë¦„**ì„ ê³ ë¯¼í•˜ë©° êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.
* ì‹¤ë¬´ì—ì„œ ìµìˆ™í–ˆë˜ Airflow íƒœìŠ¤í¬ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, **ì‘ì—…ì„ ëª¨ë“ˆí™”í•˜ê³  ìœ ì—°í•˜ê²Œ ì—°ê²°í•˜ëŠ” ë°©ì‹**ì„ ì •ì œí–ˆìŠµë‹ˆë‹¤.
* ê¸°ì¡´ ì½”ë“œë¥¼ ëŒì•„ë³´ë©°, **ì¬ì‚¬ìš©ì„±ê³¼ í™•ì¥ì„±**ì„ ê³ ë ¤í•œ êµ¬ì¡°ë¡œ ê°œì„ í•˜ëŠ” ë° ì¤‘ì ì„ ë‘ì—ˆìŠµë‹ˆë‹¤.


<br/>
<br/>
<br/>

---
<br/>
<br/>

# ğŸ¯ PERSONAL_AIRFLOW

> **"Automate the entire data flow and extract insights."**

This project is a **user analysis pipeline with automated ETL**, built on top of Apache Airflow.
It covers the full flow from **AWS S3 â†’ data preprocessing â†’ analysis â†’ BigQuery loading**.

Additionally, it incorporates GA4 logs to compute **engagement metrics (WAU/MAU)** and **conversion rates**.

<br/>

---

## ğŸ§© Components

* **Airflow DAG**: Orchestrates the entire analysis flow (`service_data_pipeline.py`)
* **S3 Integration**: Loads and preprocesses membership CSV data
* **User Analysis**: Calculates metrics such as signups, payments, churn, retention
* **GA4 Integration**: Runs external queries (`.sql`) to extract key engagement indicators
* **BigQuery Loading**: Stores results in weekly/monthly tables

<br/>

---

## ğŸ—‚ Project Structure

```
PERSONAL_AIRFLOW/
â”œâ”€â”€ dags/              # DAG definitions
â”‚   â”œâ”€â”€ blog_data_pipeline.py
â”‚   â”œâ”€â”€ blog_member_management.py
â”‚   â””â”€â”€ service_data_pipeline.py
â”œâ”€â”€ queries/           # External GA4 SQL queries
â”‚   â”œâ”€â”€ mau_query.sql
â”‚   â””â”€â”€ wau_query.sql
â”œâ”€â”€ scripts/           # Initialization scripts
â”‚   â””â”€â”€ init_airflow_connections.sh
â”œâ”€â”€ config/            # Example config files
â”œâ”€â”€ auth/              # Authentication files (excluded from repo)
â”œâ”€â”€ plugins/           # Optional custom plugins
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ entrypoint.sh
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

> This repository includes only `dags/`, `queries/`, and `requirements.txt`.

<br/>

---

## ğŸ” Data Flow Diagram

Actual Airflow DAG execution result:
![Airflow DAG](https://github.com/user-attachments/assets/c362c2be-0376-4d9b-9a56-8d2d6265358d)

<br/>

Alternatively, a Mermaid diagram representation:

```mermaid
graph TB
  A[S3: CSV Upload] --> B[Extract Task]
  B --> C[Datetime Preprocessing]
  C --> D1[Weekly Analysis]
  C --> D2[Monthly Analysis]
  D1 --> E1[GA4 WAU Query]
  D2 --> E2[GA4 MAU Query]
  E1 --> F1[Conversion Analysis]
  E2 --> F2[Conversion Analysis]
  F1 --> G1[Load to BigQuery: Weekly]
  F2 --> G2[Load to BigQuery: Monthly]
```

<br/>

---

## ğŸ›  Tech Stack

<img src="https://img.shields.io/badge/Python-3776AB?style=rounded&logo=Python&logoColor=white" height="25"/> <img src="https://img.shields.io/badge/Airflow-017CEE?style=rounded&logo=Apache%20Airflow&logoColor=white" height="25"/> <img src="https://img.shields.io/badge/Amazon%20S3-569A31?style=rounded&logo=Amazon%20S3&logoColor=white" height="25"/> <img src="https://img.shields.io/badge/BigQuery-1A73E8?style=rounded&logo=Google%20BigQuery&logoColor=white" height="25"/> <img src="https://img.shields.io/badge/Docker-0db7ed?style=rounded&logo=Docker&logoColor=white" height="25"/> <img src="https://img.shields.io/badge/SQL-4479A1?style=rounded&logo=SQLite&logoColor=white" height="25"/> <img src="https://img.shields.io/badge/%2B%20more-8E44AD?style=rounded&logoColor=white" height="25"/>

<br/>

---

## ğŸ“¦ Additional DAGs Overview

### `blog_data_pipeline.py`
> Collects blog members and newsletters data from the Ghost Admin API,  
> and calculates key weekly metrics such as subscriber count, active users, subscription rate, and newsletter engagement KPIs (open/click/delivery rates).  
> - Raw data is stored in both S3 and BigQuery, ready for use in dashboards or reporting pipelines.  
> - Data is processed separately for Korean and English audiences to provide language-specific insights.

### `blog_member_management.py`
> Automates management of the Ghost blog membership system.  
> - Inactive or low-engagement users are programmatically deleted,  
> - While new recommended (pending) users from BigQuery are automatically created in Ghost.  
> - Ensures data consistency between Ghost and BigQuery by running regular sync and upsert operations.

<br/>

---

## âœï¸ Project Intent

* Designed to improve efficiency for repetitive analytics tasks frequently encountered in real-world scenarios.
* This is a self-directed project aimed at integrating analysis and engineering through Apache Airflow.
* Covers the full journey from preprocessing to log-based analysis and conversion rate calculation, focusing on **a cohesive data workflow**.
* Refines task orchestration using modular and maintainable patterns familiar from actual projects.
* Emphasizes **reusability and scalability** in structure while reflecting on and improving existing code.
